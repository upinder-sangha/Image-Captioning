# Image-Captioning

Generating captions for images is a challenging task in the field of computer vision and natural language processing. In this project, I propose a solution to this problem by leveraging the power of deep learning techniques. Specifically, using the ResNet architecture to extract features from images, and GloVe word embeddings to map the words to their embedding vectors, which are then fed into a long short-term memory (LSTM) model. The model is trained on the Flickr8 dataset, a widely used benchmark dataset for image captioning, and evaluated using the BLEU score Metric on which the model obtained a score of 0.596 for BLEU-1 and 0.174 for BLEU-4. Experimental results showed that this approach achieves competitive performance compared to state-of-the-art methods, demonstrating the effectiveness of the model in generating high-quality image captions.
