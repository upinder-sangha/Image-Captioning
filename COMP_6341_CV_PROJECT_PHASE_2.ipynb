{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/upinder-sangha/Image-Captioning/blob/main/COMP_6341_CV_PROJECT_PHASE_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Phase 2: Training the model to generate captions"
      ],
      "metadata": {
        "id": "BwzSo5mZvmz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Getting the test and train sets from phase 1"
      ],
      "metadata": {
        "id": "dYDY6vF4vwGy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubRwFMuOtSUn",
        "outputId": "480cfaef-0814-4068-97d5-a4de5e025f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "jHZIFAuVvrkf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp drive/MyDrive/COMP_6341_CV_PROJECT_RES/features.pkl /content\n",
        "# !cp drive/MyDrive/COMP_6341_CV_PROJECT_RES/captions.pkl /content\n",
        "\n",
        "# captions = pickle.load(open(\"captions.pkl\",\"rb\"))\n",
        "# features = pickle.load(open(\"features.pkl\",\"rb\"))"
      ],
      "metadata": {
        "id": "rmUb96XiuuOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/COMP_6341_CV_PROJECT_RES/train_features.pkl /content\n",
        "!cp drive/MyDrive/COMP_6341_CV_PROJECT_RES/test_features.pkl /content\n",
        "!cp drive/MyDrive/COMP_6341_CV_PROJECT_RES/train_captions.pkl /content\n",
        "!cp drive/MyDrive/COMP_6341_CV_PROJECT_RES/test_captions.pkl /content\n",
        "\n",
        "train_features = pickle.load(open(\"train_features.pkl\",\"rb\"))\n",
        "test_features = pickle.load(open(\"test_features.pkl\",\"rb\"))\n",
        "train_captions = pickle.load(open(\"train_captions.pkl\",\"rb\"))\n",
        "test_captions = pickle.load(open(\"test_captions.pkl\",\"rb\"))\n",
        "\n",
        "print(\"size of training set = \", len(train_captions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RziwDtu503EF",
        "outputId": "ad469f80-5dae-40ee-ff9a-5ec70e26f755"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of training set =  7120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing the data to match the format of the model"
      ],
      "metadata": {
        "id": "1A5xkoaNv9gj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining the tokenizer"
      ],
      "metadata": {
        "id": "GRngM6_wwJH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 6000\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "# convert a dictionary of clean captions to a list of captions\n",
        "def to_list(captions):\n",
        "    list_of_all_captions = list()\n",
        "    for key in captions.keys():\n",
        "       [list_of_all_captions.append(caption) for caption in captions[key]]\n",
        "    return list_of_all_captions\n",
        " \n",
        "# fit a tokenizer given a caption\n",
        "def create_tokenizer(captions):\n",
        "    list_of_all_captions = to_list(captions)\n",
        "    tokenizer = Tokenizer(num_words=vocab_size)\n",
        "    tokenizer.fit_on_texts(list_of_all_captions)\n",
        "    return tokenizer\n",
        " \n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_captions)\n",
        "print('Number of unique words: %d' % (len(tokenizer.word_index)))\n",
        "print('Vocab Size: %d' % vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6T3D4L51KYN",
        "outputId": "cf1d31ac-2866-4612-e959-479573e24cbf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words: 7955\n",
            "Vocab Size: 6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save the tokenizer for next phase\n",
        "pickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
        "!cp tokenizer.pkl drive/MyDrive/COMP_6341_CV_PROJECT_RES/"
      ],
      "metadata": {
        "id": "Vb_P8ynBYQOk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###finding and saving the maximum length of any caption in training data"
      ],
      "metadata": {
        "id": "YvgYyPbmweUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the length of the description with the most words\n",
        "def max_length(captions):\n",
        "    list_of_all_captions = to_list(captions)\n",
        "    return max(len(caption.split()) for caption in list_of_all_captions)"
      ],
      "metadata": {
        "id": "N9y3Waay3DcP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max_length(train_captions)\n",
        "print(max_length)\n",
        "\n",
        "pickle.dump(max_length, open('max_length.pkl', 'wb'))\n",
        "!cp max_length.pkl drive/MyDrive/COMP_6341_CV_PROJECT_RES/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIV9EUrXDT8T",
        "outputId": "e5d364dd-a88f-4a6d-c327-4f3eba56ff1f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create inverse index mapping numbers to words\n",
        "index_to_word = {v: k for k, v in tokenizer.word_index.items()}"
      ],
      "metadata": {
        "id": "YdPsZ5-5Fuhz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mapping the words to their 50 dimentional glove word embeddings "
      ],
      "metadata": {
        "id": "vWEm8NJkw7Wx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgCosvmfHGUD",
        "outputId": "ae36a295-148f-4cea-c784-15645b5e399e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-12 02:51:11--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-04-12 02:51:11--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-04-12 02:51:11--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.13MB/s    in 2m 39s  \n",
            "\n",
            "2023-04-12 02:53:51 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/             glove.6B.50d.txt  test_captions.pkl   train_features.pkl\n",
            "glove.6B.100d.txt  glove.6B.zip      test_features.pkl\n",
            "glove.6B.200d.txt  max_length.pkl    tokenizer.pkl\n",
            "glove.6B.300d.txt  \u001b[01;34msample_data\u001b[0m/      train_captions.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This piece of code was referenced from an article \"How to Use Word Embedding Layers for Deep Learning with Keras\" \n",
        "#written by  Jason Brownlee on October 4, 2017 in Deep Learning for Natural Language Processing. https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "embeddings = {} # We create a dictionary of word -> embedding\n",
        "\n",
        "with open(\"glove.6B.50d.txt\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0] # The first value is the word, the rest are the values of the embedding\n",
        "        embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n",
        "        embeddings[word] = embedding # Add embedding to our embedding dictionary\n",
        "\n",
        "print('Found {:,} word vectors in GloVe.'.format(len(embeddings)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyPf2hrCHG7m",
        "outputId": "489f906f-865a-425c-cdf6-12b33c043369"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400,000 word vectors in GloVe.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (embeddings['frog'])\n",
        "print (len(embeddings['frog']))\n",
        "print(type(embeddings['frog']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN5vvByTHec5",
        "outputId": "09af8dd4-85fc-462b-aa98-58c4d59e0e49"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.61038  -0.20757  -0.71951   0.89304   0.32482   0.76564   0.1814\n",
            " -0.33086   0.79173  -0.31664   0.011143  0.45412   1.5992    0.013494\n",
            " -0.093646  0.19245   0.251     1.1277   -1.0897   -0.42909  -1.1327\n",
            " -0.90465   0.5617   -0.058464  1.0007   -0.39017  -0.41665   0.73721\n",
            " -0.53824  -0.95993   0.67929  -0.59053   0.13408   0.54273  -0.36615\n",
            "  0.014978 -0.2496   -0.81088   0.078905 -0.97552  -0.66394  -0.18508\n",
            " -0.87174   0.30782   1.2839   -0.14884   0.62178  -1.509     0.14582\n",
            " -0.31682 ]\n",
            "50\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "id": "_VpK9qbVJUpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This piece of code was referenced from an article \"How to Use Word Embedding Layers for Deep Learning with Keras\" \n",
        "#written by  Jason Brownlee on October 4, 2017 in Deep Learning for Natural Language Processing. https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "\n",
        "embedding_dim = 50 # We use 50 dimensional glove vectors\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(vocab_size, len(word_index)) # How many words are there actually\n",
        "\n",
        "embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
        "\n",
        "# The vectors need to be in the same position as their index. \n",
        "\n",
        "# Loop over all words in the word index\n",
        "for word, i in word_index.items():\n",
        "    # If we are above the amount of words we want to use we do nothing\n",
        "    if i >= vocab_size: \n",
        "        continue\n",
        "    # Get the embedding vector for the word\n",
        "    embedding_vector = embeddings.get(word)\n",
        "    # If there is an embedding vector, put it in the embedding matrix\n",
        "    if embedding_vector is not None: \n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "JM-wD96pEmEy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"word at index 5: \",index_to_word.get(5))\n",
        "print(\"embedding at index 5: \\n\",embedding_matrix[5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imJDo5w-T2dF",
        "outputId": "a71d5520-09c3-451a-9c4c-58cd8a4b34d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word at index 5:  the\n",
            "embedding at index 5: \n",
            " [ 4.18000013e-01  2.49679998e-01 -4.12420005e-01  1.21699996e-01\n",
            "  3.45270008e-01 -4.44569997e-02 -4.96879995e-01 -1.78619996e-01\n",
            " -6.60229998e-04 -6.56599998e-01  2.78430015e-01 -1.47670001e-01\n",
            " -5.56770027e-01  1.46579996e-01 -9.50950012e-03  1.16579998e-02\n",
            "  1.02040000e-01 -1.27920002e-01 -8.44299972e-01 -1.21809997e-01\n",
            " -1.68009996e-02 -3.32789987e-01 -1.55200005e-01 -2.31309995e-01\n",
            " -1.91809997e-01 -1.88230002e+00 -7.67459989e-01  9.90509987e-02\n",
            " -4.21249986e-01 -1.95260003e-01  4.00710011e+00 -1.85939997e-01\n",
            " -5.22870004e-01 -3.16810012e-01  5.92130003e-04  7.44489999e-03\n",
            "  1.77780002e-01 -1.58969998e-01  1.20409997e-02 -5.42230010e-02\n",
            " -2.98709989e-01 -1.57490000e-01 -3.47579986e-01 -4.56370004e-02\n",
            " -4.42510009e-01  1.87849998e-01  2.78489990e-03 -1.84110001e-01\n",
            " -1.15139998e-01 -7.85809994e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Splitting the training set into train and validation set"
      ],
      "metadata": {
        "id": "xbtsbF00yDiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def train_val_split(train_plus_val_captions,train_plus_val_features):\n",
        "\n",
        "    train_plus_val_id_set = set(train_plus_val_captions.keys())\n",
        "    train_validation_ratio = 0.9\n",
        "\n",
        "    num_of_train = int(len(train_plus_val_id_set) * train_validation_ratio)\n",
        "\n",
        "    # Randomly select elements for the train set\n",
        "    train_id_set = set(random.sample(train_plus_val_id_set, num_of_train))\n",
        "\n",
        "    # Create the validation set by removing the elements in the train set\n",
        "    val_id_set = train_plus_val_id_set - train_id_set\n",
        "\n",
        "    train_features = {k: train_plus_val_features[k] for k in train_id_set}\n",
        "    val_features = {k: train_plus_val_features[k] for k in val_id_set}\n",
        "    train_captions = {k: train_plus_val_captions[k] for k in train_id_set}\n",
        "    val_captions = {k: train_plus_val_captions[k] for k in val_id_set}\n",
        "\n",
        "    return train_captions, train_features, val_captions, val_features"
      ],
      "metadata": {
        "id": "VmcW7LECqU4-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_captions, train_features, val_captions, val_features = train_val_split(train_captions,train_features)\n",
        "print(len(train_captions),len(val_captions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWONoC-ksip0",
        "outputId": "ddcbe02c-06eb-4ea5-8f37-419504e04cdd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6408 712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-7c5632767f5b>:10: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  train_id_set = set(random.sample(train_plus_val_id_set, num_of_train))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defining our deeplearning model"
      ],
      "metadata": {
        "id": "stDxFfieyMQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import add\n",
        "from keras.layers import concatenate\n",
        "from keras.layers import Reshape\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import SpatialDropout1D"
      ],
      "metadata": {
        "id": "smK4okBmx0vG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the captioning model\n",
        "def define_model4_1(vocab_size, max_length):\n",
        "    # feature extractor model\n",
        "    inputs1 = Input(shape=(2048,))\n",
        "    image_layer_1 = Dropout(0.2)(inputs1)\n",
        "    image_layer_2 = Dense(256, activation='relu')(image_layer_1)\n",
        "\n",
        "    # sequence model\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    text_layer_1 = Embedding(vocab_size, embedding_dim, weights = [embedding_matrix], trainable = False)(inputs2)\n",
        "    text_layer_2 = Dense(256, activation='leaky_relu')(text_layer_1)\n",
        "\n",
        "    # decoder model\n",
        "    decoder1 = add([image_layer_2, text_layer_2])\n",
        "    decoder2 = LSTM(256,dropout=0.5)(decoder1)\n",
        "    decoder3 = Dense(516, activation='leaky_relu')(decoder2)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder3)\n",
        "\n",
        "    # tie it together [image, seq] [word]\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    \n",
        "    # summarize model\n",
        "    print(model.summary())\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    return model\n",
        "\n",
        "model4_1 = define_model4_1(vocab_size, max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kztv_Ycc7MRB",
        "outputId": "87812247-ca3b-4f66-af59-28de35656628"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 2048)]       0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 22)]         0           []                               \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 2048)         0           ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 22, 50)       300000      ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 256)          524544      ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 22, 256)      13056       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 22, 256)      0           ['dense_1[0][0]',                \n",
            "                                                                  'dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 256)          525312      ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 516)          132612      ['lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 6000)         3102000     ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,597,524\n",
            "Trainable params: 4,297,524\n",
            "Non-trainable params: 300,000\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining the custom data generator class\n",
        "It generates batches of data to train on for the model"
      ],
      "metadata": {
        "id": "8mWX-shHyWiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.utils import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class CustomDataGen(tf.keras.utils.Sequence):\n",
        "    \n",
        "    def __init__(self, captions, features, tokenizer, max_length, vocab_size, batch_Size, shuffle=True):\n",
        "        \n",
        "        self.captions = captions\n",
        "        self.features = features\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.shuffle = shuffle\n",
        "        self.batch_size = batch_Size\n",
        "        self.keys = list(captions.keys())\n",
        "\n",
        "        self.n = len(self.keys)\n",
        "\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            random.shuffle(self.keys)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        start = index*self.batch_size\n",
        "        end = (index+1)*self.batch_size\n",
        "        batch_keys = self.keys[index*self.batch_size : (index+1)*self.batch_size]\n",
        "        \n",
        "        # X1,X2,y = create_sequences(batch_keys)\n",
        "        X1, X2, y = list(), list(), list()\n",
        "\n",
        "        for key in batch_keys:\n",
        "            caption_list = self.captions[key]\n",
        "            # walk through each caption for the image\n",
        "            tokenized_caption_list = self.tokenizer.texts_to_sequences(caption_list)\n",
        "            for tokenized_caption in tokenized_caption_list:\n",
        "                # split one tokenized caption into multiple X,y pairs\n",
        "                for i in range(1, len(tokenized_caption)):\n",
        "                    # split into input and output pair\n",
        "                    in_seq, out_word = tokenized_caption[:i], tokenized_caption[i]\n",
        "                    # pad input sequence\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n",
        "                    # encode output sequence\n",
        "                    out_word = to_categorical([out_word], num_classes=self.vocab_size)[0]\n",
        "                    # out_word = embedding_matrix[out_word]\n",
        "                    # store\n",
        "                    X1.append(self.features[key][0])\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_word)\n",
        "        return [np.array(X1), np.array(X2)], np.array(y)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n // self.batch_size"
      ],
      "metadata": {
        "id": "yaYWlyzE0rr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = CustomDataGen(train_captions, train_features,tokenizer, max_length,vocab_size,10)\n",
        "validation_generator = CustomDataGen(val_captions, val_features,tokenizer, max_length,vocab_size,10)"
      ],
      "metadata": {
        "id": "xgF8uEsZMHiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y= train_generator[0]\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ink1IWaOSaw-",
        "outputId": "6b091844-aad9-4813-8294-17476be357b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training the model"
      ],
      "metadata": {
        "id": "WOSBw5Vuyllr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard"
      ],
      "metadata": {
        "id": "pfLbUookDkr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = 'model-ep{epoch:02d}.h5'\n",
        "mcp = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False)\n",
        "es = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=5, verbose=1)\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
        "tb = TensorBoard('logs')\n",
        "\n",
        "model4_1.fit(train_generator, epochs=50,callbacks=[es, rlr,mcp, tb], validation_data=validation_generator, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU-A9i_9PA9S",
        "outputId": "a7eb36d3-ae43-4a1c-eedb-fab21ed862b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "639/640 [============================>.] - ETA: 0s - loss: 4.4248\n",
            "Epoch 1: saving model to model-ep01.h5\n",
            "640/640 [==============================] - 43s 53ms/step - loss: 4.4233 - val_loss: 3.7305 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "640/640 [==============================] - ETA: 0s - loss: 3.4961\n",
            "Epoch 2: saving model to model-ep02.h5\n",
            "640/640 [==============================] - 32s 50ms/step - loss: 3.4961 - val_loss: 3.3962 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "639/640 [============================>.] - ETA: 0s - loss: 3.2072\n",
            "Epoch 3: saving model to model-ep03.h5\n",
            "640/640 [==============================] - 32s 49ms/step - loss: 3.2074 - val_loss: 3.2582 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "639/640 [============================>.] - ETA: 0s - loss: 3.0341\n",
            "Epoch 4: saving model to model-ep04.h5\n",
            "640/640 [==============================] - 32s 50ms/step - loss: 3.0336 - val_loss: 3.1784 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "639/640 [============================>.] - ETA: 0s - loss: 2.9073\n",
            "Epoch 5: saving model to model-ep05.h5\n",
            "640/640 [==============================] - 32s 50ms/step - loss: 2.9073 - val_loss: 3.1467 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "640/640 [==============================] - ETA: 0s - loss: 2.8027\n",
            "Epoch 6: saving model to model-ep06.h5\n",
            "640/640 [==============================] - 31s 49ms/step - loss: 2.8027 - val_loss: 3.1112 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "640/640 [==============================] - ETA: 0s - loss: 2.7166\n",
            "Epoch 7: saving model to model-ep07.h5\n",
            "640/640 [==============================] - 31s 49ms/step - loss: 2.7166 - val_loss: 3.1047 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "639/640 [============================>.] - ETA: 0s - loss: 2.6434\n",
            "Epoch 8: saving model to model-ep08.h5\n",
            "640/640 [==============================] - 32s 50ms/step - loss: 2.6435 - val_loss: 3.1085 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "640/640 [==============================] - ETA: 0s - loss: 2.5762\n",
            "Epoch 9: saving model to model-ep09.h5\n",
            "640/640 [==============================] - 33s 52ms/step - loss: 2.5762 - val_loss: 3.1037 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "639/640 [============================>.] - ETA: 0s - loss: 2.5161\n",
            "Epoch 10: saving model to model-ep10.h5\n",
            "640/640 [==============================] - 35s 55ms/step - loss: 2.5164 - val_loss: 3.1028 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "639/640 [============================>.] - ETA: 0s - loss: 2.4644\n",
            "Epoch 11: saving model to model-ep11.h5\n",
            "640/640 [==============================] - 31s 49ms/step - loss: 2.4649 - val_loss: 3.1245 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "640/640 [==============================] - ETA: 0s - loss: 2.4194\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 12: saving model to model-ep12.h5\n",
            "640/640 [==============================] - 32s 50ms/step - loss: 2.4194 - val_loss: 3.1341 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "639/640 [============================>.] - ETA: 0s - loss: 2.2849\n",
            "Epoch 13: saving model to model-ep13.h5\n",
            "640/640 [==============================] - 34s 54ms/step - loss: 2.2847 - val_loss: 3.1289 - lr: 5.0000e-04\n",
            "Epoch 14/50\n",
            "639/640 [============================>.] - ETA: 0s - loss: 2.2480\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 14: saving model to model-ep14.h5\n",
            "640/640 [==============================] - 31s 49ms/step - loss: 2.2479 - val_loss: 3.1469 - lr: 5.0000e-04\n",
            "Epoch 15/50\n",
            "640/640 [==============================] - ETA: 0s - loss: 2.1728\n",
            "Epoch 15: saving model to model-ep15.h5\n",
            "640/640 [==============================] - 31s 49ms/step - loss: 2.1728 - val_loss: 3.1401 - lr: 2.5000e-04\n",
            "Epoch 15: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbe800d4c40>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving models at different checkpoints to compare"
      ],
      "metadata": {
        "id": "W-g3tVmbyrVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp model-ep07.h5 drive/MyDrive/COMP_6341_CV_PROJECT_RES/\n",
        "!cp model-ep10.h5 drive/MyDrive/COMP_6341_CV_PROJECT_RES/"
      ],
      "metadata": {
        "id": "Z7zv6CsN_mrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4.save('my_model4_1.h5')\n",
        "\n",
        "!cp my_model4_1.h5 drive/MyDrive/COMP_6341_CV_PROJECT_RES/\n",
        "# !cp model-ep01.h5 drive/MyDrive/COMP_6341_CV_PROJECT_RES/\n",
        "# !cp model-ep03.h5 drive/MyDrive/COMP_6341_CV_PROJECT_RES/\n",
        "# !cp model-ep07.h5 drive/MyDrive/COMP_6341_CV_PROJECT_RES/\n",
        "# !cp model-ep11.h5 drive/MyDrive/COMP_6341_CV_PROJECT_RES/\n",
        "# !cp model-ep12.h5 drive/MyDrive/COMP_6341_CV_PROJECT_RES/\n",
        "# !cp model-ep13.h5 drive/MyDrive/COMP_6341_CV_PROJECT_RES/"
      ],
      "metadata": {
        "id": "pYfJLHr63jTX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}